{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =====================\n# Cell 1: Imports & global config\n# =====================\n\nimport os\nimport warnings\nimport logging\nfrom typing import Dict, Any, List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nimport xgboost as xgb\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n\nlogging.basicConfig(level=logging.INFO, format=\"%(message)s\")\nlogger = logging.getLogger(__name__)\n\nCONF = {\n    \"seed\": 42,\n    \"target\": \"loan_paid_back\",\n    \"train_path\": \"/kaggle/input/playground-series-s5e11/train.csv\",\n    \"test_path\": \"/kaggle/input/playground-series-s5e11/test.csv\",\n    \"orig_path\": \"/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv\",\n    \"n_folds\": 8,\n    \"num_boost_round\": 12000,\n    \"early_stopping_rounds\": 350,\n}\n\nTARGET = CONF[\"target\"]\nnp.random.seed(CONF[\"seed\"])\n\n# we’ll fill these after feature engineering\nCATS_BASE: List[str] = []\nNUMS_BASE: List[str] = []\nNEW_FEATURES: List[str] = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:56:12.492002Z","iopub.execute_input":"2025-11-23T10:56:12.492294Z","iopub.status.idle":"2025-11-23T10:56:13.886285Z","shell.execute_reply.started":"2025-11-23T10:56:12.492269Z","shell.execute_reply":"2025-11-23T10:56:13.885381Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# =====================\n# Cell 2: Load data & advanced feature engineering\n# =====================\n\ndef load_data():\n    train = pd.read_csv(CONF[\"train_path\"])\n    test = pd.read_csv(CONF[\"test_path\"])\n    orig = pd.read_csv(CONF[\"orig_path\"])\n\n    logger.info(f\"Train: {train.shape}\")\n    logger.info(f\"Test : {test.shape}\")\n    logger.info(f\"Orig : {orig.shape}\")\n    return train, test, orig\n\n\ndef create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    # Core affordability\n    df[\"income_loan_ratio\"] = df[\"annual_income\"] / (df[\"loan_amount\"] + 1)\n    df[\"loan_to_income\"] = df[\"loan_amount\"] / (df[\"annual_income\"] + 1)\n\n    # Debt metrics\n    df[\"total_debt\"] = df[\"debt_to_income_ratio\"] * df[\"annual_income\"]\n    df[\"available_income\"] = df[\"annual_income\"] * (1 - df[\"debt_to_income_ratio\"])\n    df[\"debt_burden\"] = df[\"debt_to_income_ratio\"] * df[\"loan_amount\"]\n\n    # Payment analysis (simple approximation)\n    df[\"monthly_payment\"] = df[\"loan_amount\"] * df[\"interest_rate\"] / 1200\n    df[\"payment_to_income\"] = df[\"monthly_payment\"] / (df[\"annual_income\"] / 12 + 1)\n    df[\"affordability\"] = df[\"available_income\"] / (df[\"loan_amount\"] + 1)\n\n    # Risk scoring\n    df[\"default_risk\"] = (\n        df[\"debt_to_income_ratio\"] * 0.40\n        + (850 - df[\"credit_score\"]) / 850 * 0.35\n        + df[\"interest_rate\"] / 100 * 0.25\n    )\n\n    # Credit analysis\n    df[\"credit_utilization\"] = df[\"credit_score\"] * (1 - df[\"debt_to_income_ratio\"])\n    df[\"credit_interest_product\"] = df[\"credit_score\"] * df[\"interest_rate\"] / 100\n\n    # Log transforms\n    for col in [\"annual_income\", \"loan_amount\"]:\n        df[f\"{col}_log\"] = np.log1p(df[col])\n\n    # Grade parsing\n    df[\"grade_subgrade\"] = df[\"grade_subgrade\"].astype(str)\n    df[\"grade_letter\"] = df[\"grade_subgrade\"].str[0]\n    df[\"grade_number\"] = (\n        df[\"grade_subgrade\"].str[1:]\n        .str.extract(r\"(\\d+)\", expand=False)\n        .fillna(\"0\")\n        .astype(int)\n    )\n    grade_map = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}\n    df[\"grade_rank\"] = df[\"grade_letter\"].map(grade_map).fillna(4).astype(int)\n\n    return df\n\n\n# ---- run once ----\ntrain, test, orig = load_data()\ntest[TARGET] = -1  # dummy target to concat\n\nlogger.info(\"\\n[STEP 1] Creating Enhanced Financial Features...\")\ncombine = pd.concat([train, test, orig], axis=0, ignore_index=True)\ncombine_fe = create_advanced_features(combine)\n\nCATS_BASE = [\n    \"gender\",\n    \"marital_status\",\n    \"education_level\",\n    \"employment_status\",\n    \"loan_purpose\",\n    \"grade_subgrade\",\n]\n\nNUMS_BASE = [\n    \"annual_income\",\n    \"debt_to_income_ratio\",\n    \"credit_score\",\n    \"loan_amount\",\n    \"interest_rate\",\n]\n\nNEW_FEATURES = [\n    \"income_loan_ratio\",\n    \"loan_to_income\",\n    \"total_debt\",\n    \"available_income\",\n    \"debt_burden\",\n    \"monthly_payment\",\n    \"payment_to_income\",\n    \"affordability\",\n    \"default_risk\",\n    \"credit_utilization\",\n    \"credit_interest_product\",\n    \"annual_income_log\",\n    \"loan_amount_log\",\n    \"grade_letter\",\n    \"grade_number\",\n    \"grade_rank\",\n]\n\nlogger.info(f\"Created {len(NEW_FEATURES)} new features\")\nn_train = len(train)\nn_test = len(test)\nn_orig = len(orig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:56:14.142764Z","iopub.execute_input":"2025-11-23T10:56:14.143275Z","iopub.status.idle":"2025-11-23T10:56:17.706146Z","shell.execute_reply.started":"2025-11-23T10:56:14.143246Z","shell.execute_reply":"2025-11-23T10:56:17.705405Z"}},"outputs":[{"name":"stderr","text":"Train: (593994, 13)\nTest : (254569, 12)\nOrig : (20000, 22)\n\n[STEP 1] Creating Enhanced Financial Features...\nCreated 16 new features\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =====================\n# Cell 3: Target encoding + build base tables (cats, interactions, CE)\n# =====================\n\ndef kfold_target_encode_train_valid_test(\n    Xy_train: pd.DataFrame,\n    X_valid: pd.DataFrame,\n    X_test: pd.DataFrame,\n    col: str,\n    target_col: str,\n    n_splits: int = 10,\n    seed: int = 42,\n):\n    \"\"\"\n    Fold-wise target encoding:\n      - For Xy_train: OOF-style encoding using StratifiedKFold.\n      - For X_valid & X_test: map from full Xy_train means.\n    \"\"\"\n    from sklearn.model_selection import StratifiedKFold as SKF_TE\n\n    kf = SKF_TE(n_splits=n_splits, shuffle=True, random_state=seed)\n    tr_encoded = pd.Series(index=Xy_train.index, dtype=\"float32\")\n    y = Xy_train[target_col].values\n\n    for tr_idx, val_idx in kf.split(Xy_train, y):\n        tr_slice = Xy_train.iloc[tr_idx]\n        means = tr_slice.groupby(col)[target_col].mean()\n        tr_encoded.iloc[val_idx] = Xy_train.iloc[val_idx][col].map(means).astype(\n            \"float32\"\n        )\n\n    global_means = Xy_train.groupby(col)[target_col].mean()\n    val_encoded = X_valid[col].map(global_means).astype(\"float32\")\n    test_encoded = X_test[col].map(global_means).astype(\"float32\")\n\n    global_avg = Xy_train[target_col].mean().astype(\"float32\")\n    tr_encoded.fillna(global_avg, inplace=True)\n    val_encoded.fillna(global_avg, inplace=True)\n    test_encoded.fillna(global_avg, inplace=True)\n\n    return tr_encoded.values, val_encoded.values, test_encoded.values\n\n\ndef build_base_tables(\n    combine_fe: pd.DataFrame,\n    n_train: int,\n    n_test: int,\n    n_orig: int,\n    use_quantile_bins: bool = False,\n):\n    \"\"\"\n    One-time construction of:\n      - numeric_cat features\n      - interaction features\n      - count-encoding\n    Shared across all XGBoost experiments that use factorize() (your best configs).\n    \"\"\"\n    combine = combine_fe.copy()\n\n    CATS = CATS_BASE.copy()\n    NUMS = NUMS_BASE + [f for f in NEW_FEATURES if f not in [\"grade_letter\"]]\n    CATS.append(\"grade_letter\")\n\n    # numeric -> categorical\n    CATS_NUM: List[str] = []\n    if use_quantile_bins:\n        logger.info(\"Using quantile bins for numeric _cat features...\")\n        for c in NUMS:\n            n = f\"{c}_cat\"\n            est = KBinsDiscretizer(\n                n_bins=50,\n                encode=\"ordinal\",\n                strategy=\"quantile\",\n            )\n            combine[n] = est.fit_transform(combine[[c]]).astype(\"int32\")\n            CATS_NUM.append(n)\n    else:\n        logger.info(\"Using factorize() for numeric _cat features...\")\n        for c in NUMS:\n            n = f\"{c}_cat\"\n            combine[n], _ = combine[c].factorize()\n            combine[n] = combine[n].astype(\"int32\")\n            CATS_NUM.append(n)\n    logger.info(f\"Created {len(CATS_NUM)} numeric _cat features\")\n\n    # interactions\n    important_pairs = [\n        (\"employment_status\", \"grade_subgrade\"),\n        (\"employment_status\", \"education_level\"),\n        (\"employment_status\", \"loan_purpose\"),\n        (\"grade_subgrade\", \"loan_purpose\"),\n        (\"grade_subgrade\", \"education_level\"),\n        (\"marital_status\", \"employment_status\"),\n    ]\n    for num_cat in [\"credit_score_cat\", \"debt_to_income_ratio_cat\", \"interest_rate_cat\"]:\n        for cat in [\"employment_status\", \"grade_subgrade\"]:\n            important_pairs.append((num_cat, cat))\n\n    CATS_INTER: List[str] = []\n    for c1, c2 in important_pairs:\n        name = f\"{c1}_{c2}\"\n        if c1 in combine.columns and c2 in combine.columns:\n            combine[name] = combine[c1].astype(str) + \"_\" + combine[c2].astype(str)\n            CATS_INTER.append(name)\n    logger.info(f\"Created {len(CATS_INTER)} strategic interactions\")\n\n    # count-encoding\n    CE: List[str] = []\n    ALL_CATS = CATS + CATS_NUM + CATS_INTER\n    logger.info(f\"\\nCreating count encoding for {len(ALL_CATS)} categorical features...\")\n    for c in tqdm(ALL_CATS, desc=\"Count encoding\"):\n        tmp = combine.groupby(c)[TARGET].count()\n        tmp.name = f\"CE_{c}\"\n        CE.append(tmp.name)\n        combine = combine.merge(tmp, on=c, how=\"left\")\n    logger.info(f\"Created {len(CE)} count encodings\")\n\n    train_full = combine.iloc[:n_train].copy()\n    test_full = combine.iloc[n_train:n_train + n_test].copy()\n    orig_full = combine.iloc[-n_orig:].copy()\n\n    FEATURES = NUMS + CATS + CATS_NUM + CATS_INTER + CE\n    logger.info(\n        f\"\\nTrain_full: {train_full.shape}, Test_full: {test_full.shape}, Orig_full: {orig_full.shape}\"\n    )\n    logger.info(f\"Total FEATURES: {len(FEATURES)}\")\n\n    return train_full, test_full, orig_full, FEATURES, CATS, CATS_NUM, CATS_INTER\n\n\n# ---- build shared tables once (factorize, no quantile bins, since A & F both use that) ----\ntrain_full, test_full, orig_full, FEATURES, CATS, CATS_NUM, CATS_INTER = build_base_tables(\n    combine_fe, n_train, n_test, n_orig, use_quantile_bins=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:56:17.707487Z","iopub.execute_input":"2025-11-23T10:56:17.707811Z","iopub.status.idle":"2025-11-23T10:56:51.235924Z","shell.execute_reply.started":"2025-11-23T10:56:17.707785Z","shell.execute_reply":"2025-11-23T10:56:51.235009Z"}},"outputs":[{"name":"stderr","text":"Using factorize() for numeric _cat features...\nCreated 20 numeric _cat features\nCreated 12 strategic interactions\n\nCreating count encoding for 39 categorical features...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Count encoding:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0990d3438a8d49d5984dccabe53b16f4"}},"metadata":{}},{"name":"stderr","text":"Created 39 count encodings\n\nTrain_full: (593994, 110), Test_full: (254569, 110), Orig_full: (20000, 110)\nTotal FEATURES: 98\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =====================\n# Cell 4: XGBoost params + single-model CV runner\n# =====================\n\nlogger.info(\"\\n[STEP 2] Defining base XGBoost parameters...\")\n\nBASE_PARAMS: Dict[str, Any] = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"auc\",\n    \"learning_rate\": 0.0095,\n    \"max_depth\": 0,\n    \"subsample\": 0.82,\n    \"colsample_bytree\": 0.72,\n    \"seed\": CONF[\"seed\"],\n    \"grow_policy\": \"lossguide\",\n    \"max_leaves\": 36,\n    \"lambda\": 4.5,\n    \"alpha\": 2.2,\n    \"max_bin\": 256,\n    \"tree_method\": \"gpu_hist\",      # change to \"hist\" if no GPU\n    \"predictor\": \"gpu_predictor\",   # change to \"auto\" if no GPU\n    \"verbosity\": 0,\n}\nfor k, v in BASE_PARAMS.items():\n    logger.info(f\"  {k}: {v}\")\n\n\ndef run_cv_model(\n    name: str,\n    params_updates: Dict[str, Any],\n    cv_type: str = \"kfold\",\n    use_orig_aug: bool = True,\n    num_boost_round: int = CONF[\"num_boost_round\"],\n    early_stopping_rounds: int = CONF[\"early_stopping_rounds\"],\n    cv_seed: int = CONF[\"seed\"],\n) -> Tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"\n    Run CV for one model config on the pre-built tables.\n    Returns: (oof_auc, oof_preds, test_preds)\n    \"\"\"\n    logger.info(\"\\n\" + \"#\" * 80)\n    logger.info(f\"MODEL: {name}\")\n    logger.info(\"#\" * 80)\n\n    y_all = train_full[TARGET].astype(int).values\n    pos = (y_all == 1).sum()\n    neg = (y_all == 0).sum()\n    scale_pos_weight = neg / pos\n    logger.info(f\"Auto scale_pos_weight: {scale_pos_weight:.3f}\")\n\n    params = BASE_PARAMS.copy()\n    params[\"scale_pos_weight\"] = scale_pos_weight\n    for k, v in params_updates.items():\n        params[k] = v\n\n    logger.info(\"Final params:\")\n    for k in sorted(params.keys()):\n        logger.info(f\"  {k}: {params[k]}\")\n\n    # CV splitter\n    if cv_type.lower() == \"kfold\":\n        logger.info(\"Using plain KFold\")\n        kf = KFold(n_splits=CONF[\"n_folds\"], shuffle=True, random_state=cv_seed)\n        split_iter = kf.split(train_full[FEATURES])\n    else:\n        logger.info(\"Using StratifiedKFold\")\n        kf = StratifiedKFold(n_splits=CONF[\"n_folds\"], shuffle=True, random_state=cv_seed)\n        split_iter = kf.split(train_full[FEATURES], y_all)\n\n    oof_preds = np.zeros(n_train, dtype=\"float32\")\n    test_preds = np.zeros(n_test, dtype=\"float32\")\n    fold_scores: List[float] = []\n    best_iters: List[int] = []\n\n    X_test_base = test_full[FEATURES].copy()\n    X_orig = orig_full[FEATURES + [TARGET]].copy()\n    TARGET_ENCODE_CATS = CATS_NUM + CATS_INTER\n\n    for fold, (tr_idx, val_idx) in enumerate(\n        tqdm(list(split_iter), total=CONF[\"n_folds\"], desc=f\"CV folds ({name})\"), 1\n    ):\n        logger.info(f\"\\n{'=' * 25}\")\n        logger.info(f\"{name} - Fold {fold}/{CONF['n_folds']}\")\n        logger.info(f\"{'=' * 25}\")\n\n        X_tr = train_full.iloc[tr_idx][FEATURES + [TARGET]].copy()\n        X_val = train_full.iloc[val_idx][FEATURES + [TARGET]].copy()\n\n        if use_orig_aug:\n            Xy_train = pd.concat([X_tr, X_orig], axis=0, ignore_index=True)\n        else:\n            Xy_train = X_tr.copy()\n\n        X_valid = X_val[FEATURES].copy()\n        y_valid = X_val[TARGET].astype(int).values\n        X_test = X_test_base.copy()\n\n        logger.info(f\"Target encoding {len(TARGET_ENCODE_CATS)} features...\")\n        for c in tqdm(TARGET_ENCODE_CATS, desc=\"Target encoding\", leave=False):\n            if c not in Xy_train.columns:\n                continue\n            tr_te, val_te, te_te = kfold_target_encode_train_valid_test(\n                Xy_train[[c, TARGET]],\n                X_valid[[c]],\n                X_test[[c]],\n                col=c,\n                target_col=TARGET,\n                n_splits=10,\n                seed=CONF[\"seed\"],\n            )\n            Xy_train[c] = tr_te\n            X_valid[c] = val_te\n            X_test[c] = te_te\n\n        # cast categoricals\n        for col in CATS_BASE + [\"grade_letter\"]:\n            if col in Xy_train.columns:\n                Xy_train[col] = Xy_train[col].astype(\"category\")\n                X_valid[col] = X_valid[col].astype(\"category\")\n                X_test[col] = X_test[col].astype(\"category\")\n\n        dtrain = xgb.DMatrix(\n            Xy_train[FEATURES],\n            label=Xy_train[TARGET].astype(int).values,\n            enable_categorical=True,\n        )\n        dvalid = xgb.DMatrix(\n            X_valid[FEATURES],\n            label=y_valid,\n            enable_categorical=True,\n        )\n        dtest = xgb.DMatrix(\n            X_test[FEATURES],\n            enable_categorical=True,\n        )\n\n        evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n        model = xgb.train(\n            params=params,\n            dtrain=dtrain,\n            num_boost_round=num_boost_round,\n            evals=evals,\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=False,\n        )\n\n        best_iter = model.best_iteration if model.best_iteration is not None else num_boost_round\n        best_iters.append(best_iter)\n\n        oof_fold = model.predict(dvalid, iteration_range=(0, best_iter + 1))\n        oof_preds[val_idx] = oof_fold\n        test_preds += model.predict(dtest, iteration_range=(0, best_iter + 1)) / CONF[\"n_folds\"]\n\n        fold_auc = roc_auc_score(y_valid, oof_fold)\n        fold_scores.append(fold_auc)\n        logger.info(f\"{name} - Fold {fold} AUC: {fold_auc:.5f}\")\n\n    overall_auc = roc_auc_score(train_full[TARGET].astype(int).values, oof_preds)\n    logger.info(\"\\n\" + \"=\" * 80)\n    logger.info(f\"{name} - OOF AUC: {overall_auc:.6f}\")\n    logger.info(f\"Fold AUCs: {[f'{s:.5f}' for s in fold_scores]}\")\n    logger.info(f\"Avg best_iter: {np.mean(best_iters):.1f}\")\n\n    return overall_auc, oof_preds, test_preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:57:12.062811Z","iopub.execute_input":"2025-11-23T10:57:12.063160Z","iopub.status.idle":"2025-11-23T10:57:12.095903Z","shell.execute_reply.started":"2025-11-23T10:57:12.063136Z","shell.execute_reply":"2025-11-23T10:57:12.095006Z"}},"outputs":[{"name":"stderr","text":"\n[STEP 2] Defining base XGBoost parameters...\n  objective: binary:logistic\n  eval_metric: auc\n  learning_rate: 0.0095\n  max_depth: 0\n  subsample: 0.82\n  colsample_bytree: 0.72\n  seed: 42\n  grow_policy: lossguide\n  max_leaves: 36\n  lambda: 4.5\n  alpha: 2.2\n  max_bin: 256\n  tree_method: gpu_hist\n  predictor: gpu_predictor\n  verbosity: 0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =====================\n# Cell 5: Train final A & F, blend 50/50 -> submission_model.csv\n# =====================\n\ny_true = train[TARGET].astype(int).values\n\n# A: factorize + KFold + orig_aug (your best single)\nauc_A, oof_A, test_A = run_cv_model(\n    name=\"A_factorize_kfold_base\",\n    params_updates={},                  # same as BASE_PARAMS\n    cv_type=\"kfold\",\n    use_orig_aug=True,\n    num_boost_round=12000,\n    early_stopping_rounds=350,\n)\n\n# F: stratified + extra regularization (min_child_weight=8, gamma=0.2)\nauc_F, oof_F, test_F = run_cv_model(\n    name=\"F_stratified_factorize_minchild8_gamma02\",\n    params_updates={\n        \"min_child_weight\": 8,\n        \"gamma\": 0.2,\n    },\n    cv_type=\"stratified\",\n    use_orig_aug=True,\n    num_boost_round=12000,\n    early_stopping_rounds=350,\n)\n\nlogger.info(f\"\\nSingle model OOF AUCs:\")\nlogger.info(f\"  A_factorize_kfold_base                    : {auc_A:.6f}\")\nlogger.info(f\"  F_stratified_factorize_minchild8_gamma02  : {auc_F:.6f}\")\n\n# 50/50 blend of OOF for reference\noof_blend = 0.5 * oof_A + 0.5 * oof_F\nauc_blend = roc_auc_score(y_true, oof_blend)\nlogger.info(f\"\\nA/F 50-50 blend OOF AUC: {auc_blend:.6f}\")\n\n# 50/50 blend of test preds (this is your final “model-based” prediction)\ntest_blend = 0.5 * test_A + 0.5 * test_F\n\n# Save base model submission\nsubmission_model = pd.DataFrame({\n    \"id\": test_full[\"id\"].values,\n    TARGET: test_blend,\n})\nsubmission_model_path = \"/kaggle/working/submission_model.csv\"\nsubmission_model.to_csv(submission_model_path, index=False)\nlogger.info(f\"\\n✓ Saved base model submission to {submission_model_path}\")\nsubmission_model.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T10:57:54.227284Z","iopub.execute_input":"2025-11-23T10:57:54.228021Z","iopub.status.idle":"2025-11-23T10:58:23.659107Z","shell.execute_reply.started":"2025-11-23T10:57:54.227984Z","shell.execute_reply":"2025-11-23T10:58:23.657997Z"}},"outputs":[{"name":"stderr","text":"\n################################################################################\nMODEL: A_factorize_kfold_base\n################################################################################\nAuto scale_pos_weight: 0.252\nFinal params:\n  alpha: 2.2\n  colsample_bytree: 0.72\n  eval_metric: auc\n  grow_policy: lossguide\n  lambda: 4.5\n  learning_rate: 0.0095\n  max_bin: 256\n  max_depth: 0\n  max_leaves: 36\n  objective: binary:logistic\n  predictor: gpu_predictor\n  scale_pos_weight: 0.25184723094496453\n  seed: 42\n  subsample: 0.82\n  tree_method: gpu_hist\n  verbosity: 0\nUsing plain KFold\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"CV folds (A_factorize_kfold_base):   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3ee7ce8f754451c9ed0f252fb9c5bec"}},"metadata":{}},{"name":"stderr","text":"\n=========================\nA_factorize_kfold_base - Fold 1/8\n=========================\nTarget encoding 32 features...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Target encoding:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1188293908.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# A: factorize + KFold + orig_aug (your best single)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m auc_A, oof_A, test_A = run_cv_model(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"A_factorize_kfold_base\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mparams_updates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# same as BASE_PARAMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/296984757.py\u001b[0m in \u001b[0;36mrun_cv_model\u001b[0;34m(name, params_updates, cv_type, use_orig_aug, num_boost_round, early_stopping_rounds, cv_seed)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         model = xgb.train(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mdtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m             _check_call(\n\u001b[0m\u001b[1;32m   2051\u001b[0m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \"\"\"\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mXGBoostError\u001b[0m: [10:58:23] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [10:58:23] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fc6e7db3f2a]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb3e95a) [0x7fc6e7dca95a]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb483cd) [0x7fc6e7dd43cd]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fc6e76ecc79]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7fc6e76ed76c]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fc6e77514f7]\n  [bt] (6) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fc6e73edef0]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fc754f41e2e]\n  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fc754f3e493]\n\n\n\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fc6e7db3f2a]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7fc6e7dd45c9]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fc6e76ecc79]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7fc6e76ed76c]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fc6e77514f7]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fc6e73edef0]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fc754f41e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fc754f3e493]\n  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x7fc754f514d8]\n\n"],"ename":"XGBoostError","evalue":"[10:58:23] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [10:58:23] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fc6e7db3f2a]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb3e95a) [0x7fc6e7dca95a]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb483cd) [0x7fc6e7dd43cd]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fc6e76ecc79]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7fc6e76ed76c]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fc6e77514f7]\n  [bt] (6) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fc6e73edef0]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fc754f41e2e]\n  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fc754f3e493]\n\n\n\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7fc6e7db3f2a]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7fc6e7dd45c9]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7fc6e76ecc79]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7fc6e76ed76c]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7fc6e77514f7]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7fc6e73edef0]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fc754f41e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fc754f3e493]\n  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x7fc754f514d8]\n\n","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# =====================\n# Cell 6: Ensemble multiple submission CSVs + your new model\n# =====================\nimport glob\n\nbase_sub_path = \"/kaggle/input/22-november-2025-ps-s5e11\"\ntarget_col = TARGET\n\n# 1) Choose which past submissions to include + weights\n#    You can adjust these weights based on LB scores.\n#\n# Example using 015,017,018,019 as in your h_blend snippet\n# plus your new model submission.\n#\nsub_sources = {\n    # \"name\": (path, weight)\n    \"model\": (\"/kaggle/working/submission_model.csv\", 0.30),   # your new A/F blend\n    \"015\": (os.path.join(base_sub_path, \"submission_015.csv\"), 0.18),\n    \"017\": (os.path.join(base_sub_path, \"submission_017.csv\"), 0.10),\n    \"018\": (os.path.join(base_sub_path, \"submission_018.csv\"), 0.20),\n    \"019\": (os.path.join(base_sub_path, \"submission_019.csv\"), 0.22),\n}\n\n# 2) Normalize weights to sum to 1\nweight_sum = sum(w for (_, w) in sub_sources.values())\nsub_sources = {k: (p, w / weight_sum) for k, (p, w) in sub_sources.items()}\n\nlogger.info(\"\\nEnsembling the following submissions:\")\nfor name, (path, w) in sub_sources.items():\n    logger.info(f\"  {name}: {path} (weight={w:.4f})\")\n\n# 3) Blend predictions\nsample = pd.read_csv(\"/kaggle/input/playground-series-s5e11/sample_submission.csv\")\nblend_pred = np.zeros(len(sample), dtype=\"float64\")\n\nfor name, (path, w) in sub_sources.items():\n    sub_df = pd.read_csv(path)\n    # assume columns: id, loan_paid_back\n    # ensure correct order by merging on id (robust)\n    merged = sample[[\"id\"]].merge(sub_df[[\"id\", target_col]], on=\"id\", how=\"left\")\n    blend_pred += w * merged[target_col].values\n\nsample[target_col] = blend_pred\n\nfinal_submission_path = \"/kaggle/working/submission_ensemble.csv\"\nsample.to_csv(final_submission_path, index=False)\n\nlogger.info(f\"\\n✓ Saved ensembled submission to {final_submission_path}\")\nsample.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}