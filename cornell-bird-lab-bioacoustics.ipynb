{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d619cb6",
   "metadata": {
    "papermill": {
     "duration": 0.004568,
     "end_time": "2025-02-11T07:36:31.591184",
     "exception": false,
     "start_time": "2025-02-11T07:36:31.586616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bird Audio Classification with EfficientNet-B7 and SpecAugment\n",
    "\n",
    "### Overview\n",
    "This project implements a deep learning-based audio classification model using **EfficientNet-B7** for bird sound classification. The approach leverages **transfer learning** and **SpecAugment** for improved feature extraction and robustness.\n",
    "\n",
    "The workflow consists of:\n",
    "- **Preprocessing Audio Data**: Converting bird sound recordings into **Mel spectrograms**.\n",
    "- **Data Augmentation**: Applying **SpecAugment** (time and frequency masking).\n",
    "- **Model Architecture**: Utilizing a **pretrained EfficientNet-B7** with a modified classifier layer.\n",
    "- **Training and Evaluation**: Implementing **mixed-precision training (AMP)** and **OneCycleLR** for optimized learning rate scheduling.\n",
    "- **Checkpointing and Resumption**: Saving the best model weights and allowing for resuming training.\n",
    "\n",
    "### Data Preprocessing\n",
    "1. **Audio Loading & Resampling**: Audio files are loaded and resampled to a fixed sample rate of **32 kHz**.\n",
    "2. **Spectrogram Generation**: Mel spectrograms are computed using **torchaudio**.\n",
    "3. **SpecAugment**: Random **time and frequency masking** is applied to improve generalization.\n",
    "4. **Resizing**: Spectrograms are resized to **224x224 pixels** and converted to **3-channel images**.\n",
    "\n",
    "### Model Architecture\n",
    "- Uses **EfficientNet-B7** pretrained on ImageNet.\n",
    "- The final classifier layer is replaced with a **fully connected layer** matching the number of bird species.\n",
    "- **Cross-entropy loss with label smoothing** is applied for better regularization.\n",
    "\n",
    "### Training Pipeline\n",
    "- Uses **AdamW optimizer** with **OneCycleLR** learning rate scheduling.\n",
    "- **Automatic Mixed Precision (AMP)** for efficient training.\n",
    "- **Checkpoints**:\n",
    "  - Loads pretrained weights on first run.\n",
    "  - Resumes training from the best saved checkpoint if available.\n",
    "  - Saves the best-performing model (`best_model.pth`).\n",
    "\n",
    "### Steps to Run the Model\n",
    "1. **Install Dependencies**: Ensure that `torch`, `torchaudio`, `torchvision`, and other required libraries are installed.\n",
    "2. **Set Configuration**: Update `config` parameters, including `data_path`, `learning_rate`, and `epochs`.\n",
    "3. **Run Training**: Execute the script to preprocess audio, train the model, and evaluate performance.\n",
    "\n",
    "### Outputs\n",
    "- **Best Model Weights**: Saved as `best_model.pth`.\n",
    "- **Training Metrics**: Loss and accuracy stored in `training_metrics.pkl`.\n",
    "- **Evaluation Results**: Displays test accuracy on unseen bird audio samples.\n",
    "\n",
    "### Key Features\n",
    "âœ… **Transfer Learning**: EfficientNet-B7 for high-performance classification.  \n",
    "âœ… **Data Augmentation**: SpecAugment to improve robustness.  \n",
    "âœ… **AMP for Speedup**: Faster and memory-efficient training.  \n",
    "âœ… **Checkpointing**: Saves and resumes best model performance.  \n",
    "âœ… **OneCycleLR**: Dynamic learning rate scheduling for improved convergence.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9705c",
   "metadata": {
    "papermill": {
     "duration": 0.003357,
     "end_time": "2025-02-11T07:36:31.598405",
     "exception": false,
     "start_time": "2025-02-11T07:36:31.595048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Research Papers & Resources  \n",
    "\n",
    "Below are the key research papers and resources that inspired and contributed to this project:  \n",
    "\n",
    "ðŸ”¹ **[PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition](https://arxiv.org/abs/1912.10211)**  \n",
    "A comprehensive study on large-scale pretrained audio neural networks, demonstrating their effectiveness in **audio pattern recognition** tasks.  \n",
    "\n",
    "ðŸ”¹ **[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)**  \n",
    "Introduces **SpecAugment**, a powerful data augmentation technique that enhances **robustness in speech recognition models** by applying time and frequency masking.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daccf584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:31.606934Z",
     "iopub.status.busy": "2025-02-11T07:36:31.606599Z",
     "iopub.status.idle": "2025-02-11T07:36:41.427788Z",
     "shell.execute_reply": "2025-02-11T07:36:41.427050Z"
    },
    "papermill": {
     "duration": 9.82748,
     "end_time": "2025-02-11T07:36:41.429476",
     "exception": false,
     "start_time": "2025-02-11T07:36:31.601996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load necessary library\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed47cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.438606Z",
     "iopub.status.busy": "2025-02-11T07:36:41.438180Z",
     "iopub.status.idle": "2025-02-11T07:36:41.442018Z",
     "shell.execute_reply": "2025-02-11T07:36:41.441347Z"
    },
    "papermill": {
     "duration": 0.009573,
     "end_time": "2025-02-11T07:36:41.443181",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.433608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Reproducibility Utilities\n",
    "# -----------------------------\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e512fad",
   "metadata": {
    "papermill": {
     "duration": 0.003625,
     "end_time": "2025-02-11T07:36:41.450899",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.447274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration\n",
    "\n",
    "The `config` dictionary stores key hyperparameters and settings for the project. Each parameter plays a crucial role in controlling how the model is trained, tested, and executed. Below is a description of each configuration parameter:\n",
    "\n",
    "| Parameter                   | Description                                                                                 | Example Value |\n",
    "|----------------------------|-----------------------------------------------------------------------------------------------|---------------|\n",
    "| `seed`                     | Random seed to ensure reproducibility. Helps produce consistent results across multiple runs. | `42`          |\n",
    "| `data_path`                | Path to the dataset. In this case, it's pointing to bioacoustics data in `.mp3` format.      | `/kaggle/input/bioacoustics-data/osa bird recordings/**/*.mp3` |\n",
    "| `test_size`                | Fraction of the dataset used for testing.                                                    | `0.1` (10%)   |\n",
    "| `val_size`                 | Fraction of the dataset used for validation.                                                 | `0.1` (10%)   |\n",
    "| `sample_rate`              | Sampling rate for audio files in Hertz (Hz).                                                 | `32000`       |\n",
    "| `duration`                 | Duration of the audio to use per file (in seconds).                                           | `60`          |\n",
    "| `num_workers`              | Number of worker threads for data loading operations. Improves data loading performance.     | `4`           |\n",
    "| `num_epochs`               | Number of training epochs (how many complete passes through the dataset).                    | `1`           |\n",
    "| `learning_rate`            | The base learning rate for the optimizer. Controls how much the model adjusts in each step.  | `1e-3`        |\n",
    "| `weight_decay`             | Regularization parameter to avoid overfitting. Applies L2 penalty on model weights.          | `1e-4`        |\n",
    "| `max_lr`                   | Maximum learning rate.                                                                        | `1e-3`        |\n",
    "| `device`                   | Specifies whether the code should use a GPU (`cuda`) or CPU for computations.                | `\"cuda:0\"` or `\"cpu\"` |\n",
    "| `initial_checkpoint_path`  | Path to the model checkpoint file to initialize training.                                     | `/kaggle/working/best_model.pth` |\n",
    "| `save_checkpoint_dir`   | Path to a saved model checkpoint to resume training from.                         | `/kaggle/working/best_model.pth` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d775b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.459361Z",
     "iopub.status.busy": "2025-02-11T07:36:41.459047Z",
     "iopub.status.idle": "2025-02-11T07:36:41.515622Z",
     "shell.execute_reply": "2025-02-11T07:36:41.514704Z"
    },
    "papermill": {
     "duration": 0.062441,
     "end_time": "2025-02-11T07:36:41.517036",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.454595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Configuration\n",
    "# -----------------------------\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Load multiple folder directories\n",
    "    \"data_path\": [\n",
    "        \"/kaggle/input/birdsong-recognition/train_audio/**/*.mp3\",\n",
    "    ],\n",
    "    \n",
    "    \"test_size\": 0.1,\n",
    "    \"val_size\": 0.1,\n",
    "    \"sample_rate\": 32000,\n",
    "    \"duration\": 60,          # seconds of audio to use per file\n",
    "    \"num_workers\": 4,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"max_lr\": 1e-3,\n",
    "    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"initial_checkpoint_path\": \"/kaggle/input/bioacoustics-model-weight/best_model.pth\",\n",
    "    \"save_checkpoint_dir\": \"/kaggle/working/best_model.pth\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a1184",
   "metadata": {
    "papermill": {
     "duration": 0.003634,
     "end_time": "2025-02-11T07:36:41.524797",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.521163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Collection and Splitting\n",
    "\n",
    "This section contains functions to **load audio files** from one or more directories and **split the dataset** into training, validation, and test sets.\n",
    "\n",
    "### `load_audio_files(path_patterns)`\n",
    "This function scans one or more provided file path patterns, extracts file paths, and assigns labels based on their parent directory. It accepts either a single glob pattern (as a string) or a list of glob patterns.\n",
    "\n",
    "**Parameters:**\n",
    "- `path_patterns` *(str or list of str)*: A file path pattern (e.g., `\"/path/to/audio/**/*.mp3\"`) or a list of such patterns to search for audio files recursively.\n",
    "\n",
    "**Returns:**\n",
    "- `df` *(pandas.DataFrame)*: A DataFrame containing:\n",
    "  - `'filepath'`: Full path of the audio file.\n",
    "  - `'label'`: The category or label of the audio file, inferred from its parent directory.\n",
    "\n",
    "### `split_data(df, test_size, val_size, random_state=42)`\n",
    "This function splits the dataset into **training, validation, and test sets**, ensuring that the splits are stratified by label.\n",
    "\n",
    "**Parameters:**\n",
    "- `df` *(pandas.DataFrame)*: The dataset containing `'filepath'` and `'label'`.\n",
    "- `test_size` *(float)*: Proportion of the dataset to allocate for testing.\n",
    "- `val_size` *(float)*: Proportion of the remaining dataset to allocate for validation.\n",
    "- `random_state` *(int, default=42)*: Seed value for reproducibility.\n",
    "\n",
    "**Returns:**\n",
    "- `train` *(pandas.DataFrame)*: Training set.\n",
    "- `val` *(pandas.DataFrame)*: Validation set.\n",
    "- `test` *(pandas.DataFrame)*: Test set.\n",
    "\n",
    "**This ensures that:**\n",
    "- The test set is `test_size` fraction of the full dataset.\n",
    "- The validation set is `val_size` fraction of the remaining data after the test split.\n",
    "- Data is stratified, meaning the label distribution remains consistent across all splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f187cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.533378Z",
     "iopub.status.busy": "2025-02-11T07:36:41.533056Z",
     "iopub.status.idle": "2025-02-11T07:36:41.538637Z",
     "shell.execute_reply": "2025-02-11T07:36:41.537775Z"
    },
    "papermill": {
     "duration": 0.011424,
     "end_time": "2025-02-11T07:36:41.539936",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.528512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Data Collection and Splitting\n",
    "# -----------------------------\n",
    "def load_audio_files(path_patterns):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns: 'filepath' and 'label'.\n",
    "    Assumes that the parent directory of each file is its label.\n",
    "    Accepts a single glob pattern (str) or a list of glob patterns.\n",
    "    \"\"\"\n",
    "    if isinstance(path_patterns, str):\n",
    "        path_patterns = [path_patterns]\n",
    "    \n",
    "    data = []\n",
    "    for pattern in path_patterns:\n",
    "        file_paths = glob.glob(pattern, recursive=True)\n",
    "        for fp in file_paths:\n",
    "            label = os.path.basename(os.path.dirname(fp))\n",
    "            data.append({'filepath': fp, 'label': label})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def split_data(df, test_size, val_size, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataframe into train, validation, and test sets.\n",
    "    Stratify based on the label.\n",
    "    \"\"\"\n",
    "    train_val, test = train_test_split(df, test_size=test_size, \n",
    "                                       stratify=df['label'], random_state=random_state)\n",
    "    train, val = train_test_split(train_val, test_size=val_size, \n",
    "                                  stratify=train_val['label'], random_state=random_state)\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661abb8",
   "metadata": {
    "papermill": {
     "duration": 0.003543,
     "end_time": "2025-02-11T07:36:41.547386",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.543843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Definition with Audio Augmentation\n",
    "\n",
    "This section defines the **`BirdAudioDataset`** class, a custom PyTorch dataset designed to process bird audio recordings, apply transformations, and generate spectrogram images suitable for deep learning models.\n",
    "\n",
    "### `BirdAudioDataset`\n",
    "This class loads audio files, converts them into **Mel spectrograms**, applies **data augmentation** (if enabled), and normalizes the data.\n",
    "\n",
    "### **Initialization (`__init__` method)**\n",
    "The dataset is initialized with key parameters:\n",
    "\n",
    "**Parameters:**\n",
    "- `df` *(pandas.DataFrame)*: A dataset containing columns `'filepath'` (audio file path) and `'label_id'` (numeric label).\n",
    "- `sample_rate` *(int)*: The target sampling rate (Hz) to which all audio files will be resampled.\n",
    "- `duration` *(int)*: The number of seconds of audio to use per file.\n",
    "- `augment` *(bool, default=False)*: Whether to apply **SpecAugment**-style data augmentation.\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "- The number of samples per file is calculated as `sample_rate * duration`.\n",
    "- **Mel Spectrogram Transformation**:\n",
    "  - Converts audio into a **Mel Spectrogram** with 128 Mel frequency bins.\n",
    "  - Converts amplitude values to **decibels (dB)** using `AmplitudeToDB()`.\n",
    "- **SpecAugment (if enabled)**:\n",
    "  - `FrequencyMasking`: Masks random frequency bands to improve generalization.\n",
    "  - `TimeMasking`: Masks random time intervals to simulate real-world distortions.\n",
    "\n",
    "### **Dataset Length (`__len__` method)**\n",
    "Returns the number of samples in the dataset:\n",
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.df)\n",
    "```\n",
    "\n",
    "### **Loading & Processing Audio (`__getitem__` method)**\n",
    "This method:\n",
    "1. **Loads the audio file** using `torchaudio.load(filepath)`.\n",
    "2. **Converts stereo to mono** (if applicable).\n",
    "3. **Resamples** to the target `sample_rate` (if needed).\n",
    "4. **Trims/Pads** the waveform to the required duration.\n",
    "5. **Generates a Mel spectrogram** and converts it to dB scale.\n",
    "6. **Applies SpecAugment transformations** (if enabled).\n",
    "7. **Normalizes and resizes** the spectrogram to **224Ã—224** pixels.\n",
    "8. **Converts the spectrogram into a 3-channel image** for compatibility with deep learning models.\n",
    "9. **Returns** the processed **spectrogram image** and the **label**.\n",
    "\n",
    "**Returns:**\n",
    "- `image` *(Tensor, shape `[3, 224, 224]`)*: A spectrogram image with 3 color channels.\n",
    "- `label` *(int)*: The corresponding label for the audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13e0843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.556024Z",
     "iopub.status.busy": "2025-02-11T07:36:41.555705Z",
     "iopub.status.idle": "2025-02-11T07:36:41.563816Z",
     "shell.execute_reply": "2025-02-11T07:36:41.563100Z"
    },
    "papermill": {
     "duration": 0.013927,
     "end_time": "2025-02-11T07:36:41.565096",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.551169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Dataset Definition with Audio Augmentation\n",
    "# -----------------------------\n",
    "class BirdAudioDataset(Dataset):\n",
    "    def __init__(self, df, sample_rate, duration, augment=False):\n",
    "        \"\"\"\n",
    "        df: DataFrame with columns 'filepath' and 'label_id'\n",
    "        sample_rate: target sample rate (Hz)\n",
    "        duration: duration (in seconds) to use from each audio file\n",
    "        augment: whether to apply SpecAugment style augmentation\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.num_samples = sample_rate * duration\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Create MelSpectrogram transform.\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_fft=1024, hop_length=512, n_mels=128\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "        # SpecAugment transforms (if augment=True)\n",
    "        if self.augment:\n",
    "            self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=15)\n",
    "            self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=30)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filepath = row['filepath']\n",
    "        \n",
    "        # Load and process audio.\n",
    "        waveform, sr = torchaudio.load(filepath)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        if waveform.shape[1] < self.num_samples:\n",
    "            padding = self.num_samples - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "        \n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec_db = self.amplitude_to_db(mel_spec)\n",
    "        \n",
    "        if self.augment:\n",
    "            mel_spec_db = self.freq_mask(mel_spec_db)\n",
    "            mel_spec_db = self.time_mask(mel_spec_db)\n",
    "            \n",
    "        # Normalize and resize.\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-9)\n",
    "        mel_spec_db = F.interpolate(mel_spec_db.unsqueeze(0), size=(224, 224),\n",
    "                                    mode='bilinear', align_corners=False).squeeze(0)\n",
    "        image = mel_spec_db.repeat(3, 1, 1)  # Convert to 3 channels\n",
    "        \n",
    "        label = row['label_id']\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50117060",
   "metadata": {
    "papermill": {
     "duration": 0.003655,
     "end_time": "2025-02-11T07:36:41.572766",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.569111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Definition (EfficientNet-B7 with Fine-Tuning)\n",
    "\n",
    "This section defines the **`BirdClassifier`** model, which fine-tunes **EfficientNet-B7** for bird sound classification. EfficientNet-B7 is a **state-of-the-art** convolutional neural network (CNN) known for its high performance on image classification tasks.\n",
    "\n",
    "### **`BirdClassifier` Class**\n",
    "This class initializes a pre-trained **EfficientNet-B7** model and modifies the classifier layer to match the number of output classes.\n",
    "\n",
    "### **Initialization (`__init__` method)**\n",
    "- Loads a pre-trained **EfficientNet-B7** model using `efficientnet_b7()`.\n",
    "- Retrieves the default **pre-trained weights** (`EfficientNet_B7_Weights.DEFAULT`).\n",
    "- Replaces the final **fully connected (FC) layer** with a new `nn.Linear` layer to match the number of output classes.\n",
    "\n",
    "**Parameters:**\n",
    "- `num_classes` *(int)*: The number of classes in the dataset.\n",
    "\n",
    "**Modifications:**\n",
    "- Extracts the number of input features from the **original classifier**.\n",
    "- Replaces the final FC layer with a new linear layer of shape **(in_features, num_classes)**.\n",
    "\n",
    "### **Forward Pass (`forward` method)**\n",
    "Defines the forward propagation of input **image tensors** through the model.\n",
    "\n",
    "- **Input**: A batch of images (`x`) with shape `[batch_size, 3, 224, 224]`.\n",
    "- **Output**: A tensor of shape `[batch_size, num_classes]`, containing class logits (before applying softmax).\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    return self.model(x)\n",
    "```\n",
    "\n",
    "### **Key Features of the Model**\n",
    "- **Uses EfficientNet-B7**, a **highly efficient** CNN with a strong accuracy-to-performance ratio.\n",
    "- **Leverages pre-trained weights**, allowing for **transfer learning**â€”reducing the need for large datasets.\n",
    "- **Replaces the classifier** to accommodate a new classification task (i.e., bird sound spectrograms).\n",
    "- **Outputs logits** for classification, which can be converted to probabilities using `torch.nn.functional.softmax`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f74eccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.581351Z",
     "iopub.status.busy": "2025-02-11T07:36:41.581065Z",
     "iopub.status.idle": "2025-02-11T07:36:41.585734Z",
     "shell.execute_reply": "2025-02-11T07:36:41.584859Z"
    },
    "papermill": {
     "duration": 0.010423,
     "end_time": "2025-02-11T07:36:41.586968",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.576545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Model Definition (EfficientNet-B7 with Fine-Tuning)\n",
    "# -----------------------------\n",
    "class BirdClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BirdClassifier, self).__init__()\n",
    "        # Use EfficientNet-B7 and its default weights.\n",
    "        weights = EfficientNet_B7_Weights.DEFAULT\n",
    "        self.model = efficientnet_b7(weights=weights)\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb9df4",
   "metadata": {
    "papermill": {
     "duration": 0.00356,
     "end_time": "2025-02-11T07:36:41.594422",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.590862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and Evaluation (with AMP and Checkpointing)\n",
    "\n",
    "This section defines functions for **training, evaluating, and testing** the model using **Automatic Mixed Precision (AMP)** for efficient computation and **checkpointing** to save the best model.\n",
    "\n",
    "### **`train_model` Function**\n",
    "This function trains the **EfficientNet-B7** model using **AdamW optimizer, OneCycle learning rate scheduling, and label smoothing** for better generalization.\n",
    "\n",
    "### **Parameters:**\n",
    "- `model` *(nn.Module)*: The PyTorch model to train.\n",
    "- `train_loader` *(DataLoader)*: DataLoader for the training dataset.\n",
    "- `val_loader` *(DataLoader)*: DataLoader for the validation dataset.\n",
    "- `device` *(str)*: `\"cuda\"` or `\"cpu\"`, based on availability.\n",
    "- `num_epochs` *(int)*: Total number of training epochs.\n",
    "- `max_lr` *(float)*: Maximum learning rate for **OneCycleLR** scheduler.\n",
    "\n",
    "### **Training Process:**\n",
    "1. **Loss & Optimization Setup**\n",
    "   - Uses **CrossEntropyLoss** with `label_smoothing=0.1` to improve generalization.\n",
    "   - Uses **AdamW optimizer** for training.\n",
    "   - Uses **OneCycleLR scheduler** for dynamic learning rate adjustments.\n",
    "   - Uses **AMP GradScaler** for mixed precision training (reducing memory and increasing speed).\n",
    "\n",
    "2. **Training Loop**\n",
    "   - Iterates through the dataset, computes loss, and updates model parameters.\n",
    "   - Uses **autocast** for mixed precision computations.\n",
    "   - Tracks **training loss and accuracy**.\n",
    "\n",
    "3. **Validation Loop**\n",
    "   - Evaluates the model on the validation set without updating weights.\n",
    "   - Tracks **validation loss and accuracy**.\n",
    "\n",
    "4. **Model Checkpointing**\n",
    "   - Saves the model if the validation accuracy improves.\n",
    "\n",
    "5. **Returns:**\n",
    "   - The **trained model**.\n",
    "   - A dictionary containing **training and validation metrics** (loss and accuracy).\n",
    "\n",
    "### **`test_model` Function**\n",
    "This function evaluates the trained model on a **test dataset**.\n",
    "\n",
    "### **Parameters:**\n",
    "- `model` *(nn.Module)*: Trained model.\n",
    "- `test_loader` *(DataLoader)*: DataLoader for the test dataset.\n",
    "- `device` *(str)*: `\"cuda\"` or `\"cpu\"`, based on availability.\n",
    "\n",
    "### **Testing Process:**\n",
    "1. Sets the model to **evaluation mode** (`model.eval()`).\n",
    "2. Iterates through the test dataset, making predictions.\n",
    "3. Computes the **test accuracy**.\n",
    "\n",
    "### **Returns:**\n",
    "- Prints the **test accuracy**.\n",
    "\n",
    "### **Key Features:**\n",
    "- **Automatic Mixed Precision (AMP)**: Uses `torch.amp.autocast()` for faster training with reduced memory consumption.\n",
    "- **OneCycle Learning Rate Scheduler**: Adjusts learning rate dynamically for stable convergence.\n",
    "- **Label Smoothing (0.1)**: Reduces overconfidence in predictions and improves generalization.\n",
    "- **Checkpointing**: Saves the best model when validation accuracy improves.\n",
    "- **Progress Tracking**: Uses `tqdm` for real-time monitoring of training progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09559332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.602847Z",
     "iopub.status.busy": "2025-02-11T07:36:41.602557Z",
     "iopub.status.idle": "2025-02-11T07:36:41.615691Z",
     "shell.execute_reply": "2025-02-11T07:36:41.614803Z"
    },
    "papermill": {
     "duration": 0.018943,
     "end_time": "2025-02-11T07:36:41.616999",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.598056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Training and Evaluation (with AMP and Checkpointing)\n",
    "# -----------------------------\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs, max_lr):\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=config[\"weight_decay\"])\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, \n",
    "                                              total_steps=total_steps,\n",
    "                                              pct_start=0.1, anneal_strategy='cos',\n",
    "                                              div_factor=25.0, final_div_factor=1e4)\n",
    "    \n",
    "    # Initialize AMP GradScaler using the updated API.\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, running_corrects, total = 0.0, 0, 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use the new autocast API with explicit device type.\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scheduler.step()\n",
    "            scaler.update()\n",
    "\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Validation phase.\n",
    "        model.eval()\n",
    "        val_running_loss, val_running_corrects, val_total = 0.0, 0, 0\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_running_corrects += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                val_pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        epoch_val_loss = val_running_loss / val_total\n",
    "        epoch_val_acc = val_running_corrects / val_total\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "        \n",
    "        # Checkpointing: Save model if validation accuracy improves.\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save(model.state_dict(), config[\"save_checkpoint_dir\"])\n",
    "            print(f\"Best model updated (Val Acc: {best_val_acc:.4f}). Checkpoint saved.\")\n",
    "    \n",
    "    metrics = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_loss': val_losses,\n",
    "        'val_acc': val_accuracies\n",
    "    }\n",
    "    return model, metrics\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944163e",
   "metadata": {
    "papermill": {
     "duration": 0.003557,
     "end_time": "2025-02-11T07:36:41.624581",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.621024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Function: Data Loading, Training, and Saving Metrics\n",
    "\n",
    "This section defines the **`main()`** function, which handles the **entire pipeline** from data loading, dataset preparation, model training, evaluation, and saving metrics.\n",
    "\n",
    "### **Overview of `main()`**\n",
    "1. **Sets the random seed** for reproducibility.\n",
    "2. **Loads and processes the dataset**, assigning unique labels to each bird species.\n",
    "3. **Splits the data** into training, validation, and test sets.\n",
    "4. **Creates DataLoaders** for efficient batch processing.\n",
    "5. **Initializes the model** with EfficientNet-B7.\n",
    "6. **Loads model checkpoints** (if available) to resume training.\n",
    "7. **Trains the model** and evaluates it on the validation dataset.\n",
    "8. **Tests the trained model** on the test dataset.\n",
    "9. **Saves training metrics** to a `.pkl` file.\n",
    "\n",
    "### **Step-by-Step Breakdown**\n",
    "\n",
    "### **1. Set Random Seed for Reproducibility**\n",
    "```python\n",
    "set_seed(config[\"seed\"])\n",
    "```\n",
    "- Ensures consistent results across multiple runs.\n",
    "\n",
    "### **2. Load and Process the Data**\n",
    "```python\n",
    "df = load_audio_files(config[\"data_path\"])\n",
    "labels_sorted = sorted(df['label'].unique())\n",
    "label2id = {label: i for i, label in enumerate(labels_sorted)}\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "print(f\"Found {len(df)} audio files across {len(labels_sorted)} classes.\")\n",
    "```\n",
    "- Loads audio file paths and assigns labels based on directory names.\n",
    "- Maps each unique label to a numeric **label ID**.\n",
    "\n",
    "### **3. Split Data into Train, Validation, and Test Sets**\n",
    "```python\n",
    "train_df, val_df, test_df = split_data(df, config[\"test_size\"], config[\"val_size\"], random_state=config[\"seed\"])\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "```\n",
    "- Uses **stratified splitting** to maintain class balance.\n",
    "\n",
    "### **4. Create Datasets and DataLoaders**\n",
    "```python\n",
    "train_dataset = BirdAudioDataset(train_df, config[\"sample_rate\"], config[\"duration\"], augment=True)\n",
    "val_dataset = BirdAudioDataset(val_df, config[\"sample_rate\"], config[\"duration\"], augment=False)\n",
    "test_dataset = BirdAudioDataset(test_df, config[\"sample_rate\"], config[\"duration\"], augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=config[\"num_workers\"])\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=config[\"num_workers\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=config[\"num_workers\"])\n",
    "```\n",
    "- **Augments training data** using SpecAugment.\n",
    "- **Creates DataLoaders** for efficient mini-batch processing.\n",
    "\n",
    "### **5. Initialize Model**\n",
    "```python\n",
    "num_classes = len(labels_sorted)\n",
    "model = BirdClassifier(num_classes)\n",
    "device = torch.device(config[\"device\"])\n",
    "```\n",
    "- Creates an **EfficientNet-B7** model.\n",
    "- Moves model to **CPU or GPU** based on availability.\n",
    "\n",
    "### **6. Load Model Checkpoints (If Available)**\n",
    "```python\n",
    "if os.path.exists(config[\"initial_checkpoint_path\"]):\n",
    "    print(f\"Existing resume checkpoint found at {config['initial_checkpoint_path']}. Loading model weights to resume training.\")\n",
    "    model.load_state_dict(torch.load(config[\"initial_checkpoint_path\"], map_location=device, weights_only=True))\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "```\n",
    "- **Resumes training** from the latest checkpoint if available.\n",
    "- **Loads pre-trained weights** if no resume checkpoint exists.\n",
    "- **Starts fresh training** if no checkpoints are found.\n",
    "\n",
    "### **7. Clear GPU Cache**\n",
    "```python\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "- Frees unused GPU memory before training starts.\n",
    "\n",
    "\n",
    "### **8. Train the Model**\n",
    "```python\n",
    "model, metrics = train_model(model, train_loader, val_loader, device,\n",
    "                             num_epochs=config[\"num_epochs\"], max_lr=config[\"max_lr\"])\n",
    "```\n",
    "- Trains the model using **OneCycleLR, label smoothing, and AMP**.\n",
    "- Returns **training metrics** (loss & accuracy).\n",
    "\n",
    "### **9. Test the Model**\n",
    "```python\n",
    "test_model(model, test_loader, device)\n",
    "```\n",
    "- Evaluates the model on the **test dataset**.\n",
    "\n",
    "### **10. Save Training Metrics**\n",
    "```python\n",
    "with open(\"/kaggle/working/training_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metrics, f)\n",
    "print(\"Training metrics saved to training_metrics.pkl\")\n",
    "```\n",
    "- Saves training and validation **loss & accuracy** for further analysis.\n",
    "\n",
    "### **Key Features:**\n",
    "âœ… **End-to-End Workflow** â€“ Handles data preparation, training, evaluation, and saving results.  \n",
    "âœ… **Model Checkpointing** â€“ Prevents loss of progress by saving the best model.  \n",
    "âœ… **GPU Optimization** â€“ Uses **AMP and CUDA** for faster and efficient training.  \n",
    "âœ… **Reproducibility** â€“ Ensures consistent results using a **fixed random seed**.  \n",
    "âœ… **Efficient Data Processing** â€“ Uses **multi-threaded DataLoaders** for faster data loading.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b76dcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.633129Z",
     "iopub.status.busy": "2025-02-11T07:36:41.632840Z",
     "iopub.status.idle": "2025-02-11T07:36:41.641756Z",
     "shell.execute_reply": "2025-02-11T07:36:41.640849Z"
    },
    "papermill": {
     "duration": 0.014753,
     "end_time": "2025-02-11T07:36:41.643019",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.628266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Main Function: Data Loading, Training, and Saving Metrics\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # Set seed for reproducibility.\n",
    "    set_seed(config[\"seed\"])\n",
    "    \n",
    "    # Load and prepare the data.\n",
    "    df = load_audio_files(config[\"data_path\"])\n",
    "    labels_sorted = sorted(df['label'].unique())\n",
    "    label2id = {label: i for i, label in enumerate(labels_sorted)}\n",
    "    df['label_id'] = df['label'].map(label2id)\n",
    "    print(f\"Found {len(df)} audio files across {len(labels_sorted)} classes.\")\n",
    "    \n",
    "    train_df, val_df, test_df = split_data(df, config[\"test_size\"], config[\"val_size\"], random_state=config[\"seed\"])\n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    \n",
    "    train_dataset = BirdAudioDataset(train_df, config[\"sample_rate\"], config[\"duration\"], augment=True)\n",
    "    val_dataset = BirdAudioDataset(val_df, config[\"sample_rate\"], config[\"duration\"], augment=False)\n",
    "    test_dataset = BirdAudioDataset(test_df, config[\"sample_rate\"], config[\"duration\"], augment=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=config[\"num_workers\"])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=config[\"num_workers\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=config[\"num_workers\"])\n",
    "    \n",
    "    num_classes = len(labels_sorted)\n",
    "    model = BirdClassifier(num_classes)\n",
    "    \n",
    "    device = torch.device(config[\"device\"])\n",
    "    \n",
    "    # ---------------\n",
    "    # Check for and load checkpoints:\n",
    "    # ---------------\n",
    "    if os.path.exists(config[\"initial_checkpoint_path\"]):\n",
    "        print(f\"Loading initial weights from {config['initial_checkpoint_path']} for the first run.\")\n",
    "        checkpoint = torch.load(config[\"initial_checkpoint_path\"], map_location=device, weights_only=True)\n",
    "        old_weight_count = checkpoint[\"model.classifier.1.weight\"].shape[0]\n",
    "        num_new_classes = num_classes\n",
    "        if num_new_classes > 0:\n",
    "            print(f\"Expanding classifier layer: adding {num_new_classes} new classes.\")\n",
    "            current_fc_weight = model.model.classifier[1].weight.data\n",
    "            current_fc_bias = model.model.classifier[1].bias.data\n",
    "            # Copy weights and biases for the old classes from the checkpoint.\n",
    "            current_fc_weight[:old_weight_count, :] = checkpoint[\"model.classifier.1.weight\"]\n",
    "            current_fc_bias[:old_weight_count] = checkpoint[\"model.classifier.1.bias\"]\n",
    "            # Update the checkpoint with the modified classifier weights.\n",
    "            checkpoint[\"model.classifier.1.weight\"] = current_fc_weight\n",
    "            checkpoint[\"model.classifier.1.bias\"] = current_fc_bias\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "    else:\n",
    "        print(\"No initial checkpoint found. Starting training from scratch.\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Train the model.\n",
    "    model, metrics = train_model(model, train_loader, val_loader, device,\n",
    "                                 num_epochs=config[\"num_epochs\"], max_lr=config[\"max_lr\"])\n",
    "    \n",
    "    # Test the model.\n",
    "    test_model(model, test_loader, device)\n",
    "    \n",
    "    # Save training metrics.\n",
    "    with open(\"/kaggle/working/training_metrics.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metrics, f)\n",
    "    print(\"Training metrics saved to training_metrics.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a93be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T07:36:41.651616Z",
     "iopub.status.busy": "2025-02-11T07:36:41.651314Z",
     "iopub.status.idle": "2025-02-11T16:46:29.692477Z",
     "shell.execute_reply": "2025-02-11T16:46:29.691112Z"
    },
    "papermill": {
     "duration": 32988.047818,
     "end_time": "2025-02-11T16:46:29.694692",
     "exception": false,
     "start_time": "2025-02-11T07:36:41.646874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21375 audio files across 264 classes.\n",
      "Train: 17313, Val: 1924, Test: 2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255M/255M [00:03<00:00, 77.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No initial checkpoint found. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Training:   0%|          | 0/542 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 5.3402, Train Acc: 0.0229 | Val Loss: 5.0483, Val Acc: 0.0598\n",
      "Best model updated (Val Acc: 0.0598). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] | Train Loss: 4.2608, Train Acc: 0.1506 | Val Loss: 4.2143, Val Acc: 0.2006\n",
      "Best model updated (Val Acc: 0.2006). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] | Train Loss: 3.5924, Train Acc: 0.2834 | Val Loss: 3.5803, Val Acc: 0.3233\n",
      "Best model updated (Val Acc: 0.3233). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] | Train Loss: 3.1569, Train Acc: 0.3884 | Val Loss: 3.3765, Val Acc: 0.3799\n",
      "Best model updated (Val Acc: 0.3799). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] | Train Loss: 2.8173, Train Acc: 0.4745 | Val Loss: 3.2606, Val Acc: 0.3992\n",
      "Best model updated (Val Acc: 0.3992). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] | Train Loss: 2.5229, Train Acc: 0.5517 | Val Loss: 3.1841, Val Acc: 0.4397\n",
      "Best model updated (Val Acc: 0.4397). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] | Train Loss: 2.2540, Train Acc: 0.6292 | Val Loss: 2.9793, Val Acc: 0.4615\n",
      "Best model updated (Val Acc: 0.4615). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] | Train Loss: 1.9844, Train Acc: 0.7113 | Val Loss: 2.9916, Val Acc: 0.4699\n",
      "Best model updated (Val Acc: 0.4699). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] | Train Loss: 1.7177, Train Acc: 0.7958 | Val Loss: 2.8997, Val Acc: 0.5036\n",
      "Best model updated (Val Acc: 0.5036). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] | Train Loss: 1.5024, Train Acc: 0.8658 | Val Loss: 2.8909, Val Acc: 0.5187\n",
      "Best model updated (Val Acc: 0.5187). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] | Train Loss: 1.3423, Train Acc: 0.9195 | Val Loss: 2.8549, Val Acc: 0.5343\n",
      "Best model updated (Val Acc: 0.5343). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] | Train Loss: 1.2384, Train Acc: 0.9514 | Val Loss: 2.7874, Val Acc: 0.5499\n",
      "Best model updated (Val Acc: 0.5499). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] | Train Loss: 1.1645, Train Acc: 0.9709 | Val Loss: 2.7265, Val Acc: 0.5530\n",
      "Best model updated (Val Acc: 0.5530). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] | Train Loss: 1.1150, Train Acc: 0.9807 | Val Loss: 2.7462, Val Acc: 0.5535\n",
      "Best model updated (Val Acc: 0.5535). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] | Train Loss: 1.0763, Train Acc: 0.9872 | Val Loss: 2.6653, Val Acc: 0.5717\n",
      "Best model updated (Val Acc: 0.5717). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] | Train Loss: 1.0529, Train Acc: 0.9897 | Val Loss: 2.6445, Val Acc: 0.5769\n",
      "Best model updated (Val Acc: 0.5769). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] | Train Loss: 1.0383, Train Acc: 0.9916 | Val Loss: 2.6389, Val Acc: 0.5722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] | Train Loss: 1.0257, Train Acc: 0.9925 | Val Loss: 2.6315, Val Acc: 0.5780\n",
      "Best model updated (Val Acc: 0.5780). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] | Train Loss: 1.0194, Train Acc: 0.9930 | Val Loss: 2.6268, Val Acc: 0.5790\n",
      "Best model updated (Val Acc: 0.5790). Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] | Train Loss: 1.0135, Train Acc: 0.9934 | Val Loss: 2.6272, Val Acc: 0.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [03:09<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5716\n",
      "Training metrics saved to training_metrics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Runs the full pipeline from data loading to model evaluation\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df921c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T16:46:32.090399Z",
     "iopub.status.busy": "2025-02-11T16:46:32.090032Z",
     "iopub.status.idle": "2025-02-11T16:46:32.097782Z",
     "shell.execute_reply": "2025-02-11T16:46:32.096935Z"
    },
    "papermill": {
     "duration": 1.140793,
     "end_time": "2025-02-11T16:46:32.098966",
     "exception": false,
     "start_time": "2025-02-11T16:46:30.958173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [5.340183579418389, 4.260805823763976, 3.5923895230775535, 3.1569192599969313, 2.8173418422095913, 2.5228826431332143, 2.253961999570725, 1.9843937424572862, 1.7177111787564117, 1.5023659721704576, 1.3423326384674867, 1.238418626639305, 1.1644722534990088, 1.115010798194342, 1.0763331298803889, 1.0529191014617, 1.0383416014280062, 1.025717053787689, 1.0193795448902099, 1.0135420569994555], 'train_acc': [0.022930745682435163, 0.15063824871483855, 0.28342863744007396, 0.388378674984116, 0.47449893143880323, 0.5516663778663432, 0.6291803846820309, 0.711257436608329, 0.7958181713163519, 0.8658233697221741, 0.9195402298850575, 0.9514237855946399, 0.97094668746029, 0.980708138393115, 0.9872350257032287, 0.9897187084849535, 0.9915670305550742, 0.9925489516548258, 0.993011032172356, 0.9933575925605037], 'val_loss': [5.048273323479412, 4.214251255542969, 3.5803158223752916, 3.376474425103709, 3.2606253666094585, 3.1840992340674767, 2.9792793981746426, 2.9915999979586214, 2.8997413884081613, 2.8909182632787314, 2.854949951667548, 2.7873524994959205, 2.7264904172901305, 2.7461740375804307, 2.6652680558623, 2.644508993799126, 2.6389263117387736, 2.631515179751073, 2.6268373983069915, 2.6272422280975785], 'val_acc': [0.059771309771309775, 0.20062370062370063, 0.3232848232848233, 0.3799376299376299, 0.3991683991683992, 0.4397089397089397, 0.46153846153846156, 0.4698544698544699, 0.5036382536382537, 0.5187110187110187, 0.5343035343035343, 0.5498960498960499, 0.553014553014553, 0.5535343035343036, 0.5717255717255717, 0.5769230769230769, 0.5722453222453222, 0.577962577962578, 0.579002079002079, 0.577962577962578]}\n"
     ]
    }
   ],
   "source": [
    "# Open the pickle file in read-binary mode\n",
    "with open(\"/kaggle/working/training_metrics.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print or inspect the data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e7527",
   "metadata": {
    "papermill": {
     "duration": 1.126902,
     "end_time": "2025-02-11T16:46:34.503932",
     "exception": false,
     "start_time": "2025-02-11T16:46:33.377030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1292430,
     "sourceId": 19596,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33010.068314,
   "end_time": "2025-02-11T16:46:38.896312",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-11T07:36:28.827998",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
